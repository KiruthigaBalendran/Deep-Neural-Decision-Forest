# -*- coding: utf-8 -*-
"""Neural Decision Forests v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gXys6G8Yh2fYwhHhU0jH5ZQnoWJarY9N

## Installing Packages
"""

# Install required packages.
import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers
import math
from tensorflow.keras.layers import StringLookup,IntegerLookup
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn import neighbors
from sklearn.metrics import confusion_matrix, classification_report, precision_score
from sklearn.model_selection import train_test_split

"""## Mount the data from drive"""

from google.colab import drive
drive.mount('/content/drive')

"""## Reading the data"""

customer_purchases = pd.read_csv('/content/drive/MyDrive/MSc/Research/Data/customer_purchases.csv')
# customer_purchases = customer_purchases.drop(['acct_typ_cd','catgy_id'], axis=1)
customer_purchases.fillna(0)
# customer_purchases.head()
len(pd.unique(customer_purchases['itm_skey']))
# customer_purchases.count()

"""## Data Preprocessing"""

# label_encoder object knows how to understand word labels.
label_encoder = preprocessing.LabelEncoder()
  
# Encode labels 
customer_purchases['cust_medal_sts']= label_encoder.fit_transform(customer_purchases['cust_medal_sts'])
customer_purchases['mkt_nm']= label_encoder.fit_transform(customer_purchases['mkt_nm'])
customer_purchases['acct_typ_cd']= label_encoder.fit_transform(customer_purchases['acct_typ_cd'])
customer_purchases['corp_typ_of_oper_cd']= label_encoder.fit_transform(customer_purchases['corp_typ_of_oper_cd'])
customer_purchases['cuisine_cd']= label_encoder.fit_transform(customer_purchases['cuisine_cd'])
customer_purchases['sts']= label_encoder.fit_transform(customer_purchases['sts'])
# customer_purchases = customer_purchases.fillna(0)

print(customer_purchases['cust_medal_sts'].unique())

customer_purchases['cust_skey'] = customer_purchases['cust_skey'].astype(int)
customer_purchases['cust_medal_sts'] = customer_purchases['cust_medal_sts'].astype(float)
customer_purchases['mkt_nm'] = customer_purchases['mkt_nm'].astype(float)
customer_purchases['x6_wk_avg_sale'] = customer_purchases['x6_wk_avg_sale'].astype(float)
customer_purchases['acct_typ_cd'] = customer_purchases['acct_typ_cd'].astype(float)
customer_purchases['corp_typ_of_oper_cd'] = customer_purchases['corp_typ_of_oper_cd'].astype(float)
customer_purchases['cuisine_cd'] = customer_purchases['cuisine_cd'].astype(float)
customer_purchases['sts'] = customer_purchases['sts'].astype(float)
customer_purchases['itm_skey'] = customer_purchases['itm_skey'].astype(int)
customer_purchases['catgy_id'] = customer_purchases['catgy_id'].astype(float)
customer_purchases['maj_id'] = customer_purchases['maj_id'].astype(float)
customer_purchases['intrm_id'] = customer_purchases['intrm_id'].astype(float)
customer_purchases['mnr_id'] = customer_purchases['mnr_id'].astype(float)
customer_purchases['cust_ordr_qty'] = customer_purchases['cust_ordr_qty'].astype(float)

print(customer_purchases.dtypes)

"""## Prepare the Data"""

X = customer_purchases
y = customer_purchases['sts']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=40)

train_data = X_train
test_data = X_test

print(f"Train dataset shape: {train_data.shape}")
print(f"Test dataset shape: {test_data.shape}")

train_data_file = "train_data.csv"
test_data_file = "test_data.csv"
TARGET_FEATURE_NAME = "sts"
TARGET_LABELS = [0,1]
learning_rate = 0.01
batch_size = 265
num_epochs = 100
num_trees = 10
depth = 10
used_features_rate = 0.8

#Derive the necessary variables
num_classes = customer_purchases['sts'].nunique()  
csv_header = list(train_data.columns)   #List of all the columns
feature_data = train_data.drop(['sts'], axis=1)
feature_names = list(feature_data)  #List of only independent variables
COLUMN_DEFAULTS = [
    [0.0] if feature_name in csv_header  else ["NA"]
    for feature_name in csv_header
]
print(csv_header)

def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):
    dataset = tf.data.experimental.make_csv_dataset(
        csv_file_path,
        batch_size=batch_size,
        column_names=csv_header,
        column_defaults=COLUMN_DEFAULTS,
        label_name=TARGET_FEATURE_NAME,
        num_epochs=1,
        header=True,
        na_value="?",
        shuffle=shuffle,
    ).map(lambda features, target: (features, target))
    return dataset.cache()

# Create one input layer for each feature
def create_model_inputs(FEATURE_NAMES):
    inputs = {}
    for feature_name in FEATURE_NAMES:
        inputs[feature_name] = layers.Input(
            name=feature_name, shape=(), dtype=tf.int64)
    return inputs

def encode_inputs(inputs):
    encoded_features = []
    for feature_name in inputs:
        encoded_feature = inputs[feature_name]
        if inputs[feature_name].shape[-1] is None:
            encoded_feature = tf.expand_dims(encoded_feature, -1)

        encoded_features.append(encoded_feature)

    encoded_features = layers.concatenate(encoded_features)
    return encoded_features

class NeuralDecisionTree(keras.Model):
    def __init__(self, depth, num_features, used_features_rate, num_classes):
        super(NeuralDecisionTree, self).__init__()
        self.depth = depth   # Pre-defined depth
        self.num_leaves = 2 ** depth  # No of leaves in the tree
        self.num_classes = num_classes  # No of classes in the dependent variable

        # Create a mask for the randomly selected features.
        # Number of features to be selected for each tree
        num_used_features = int(num_features * used_features_rate)  
        # Select "num_used_features" features from the total features
        one_hot = np.eye(num_features)
        sampled_feature_indicies = np.random.choice(
            np.arange(num_features), num_used_features, replace=False
        )
        self.used_features_mask = one_hot[sampled_feature_indicies]

        # Initialize the weights of the classes in leaves.
        self.pi = tf.Variable(
            initial_value=tf.random_normal_initializer()(
                shape=[self.num_leaves, self.num_classes]
            ),
            dtype="float32",
            trainable=True,
        )

        # Initialize the stochastic routing layer.
        self.decision_fn = layers.Dense(
            units=self.num_leaves, activation="sigmoid", name="decision"
        )

    def call(self, features):
        batch_size = tf.shape(features)[0]

        # Apply the feature mask to the input features.
        features = tf.matmul(
            features, self.used_features_mask, transpose_b=True
        )
        # Compute the routing probabilities.
        decisions = tf.expand_dims(
            self.decision_fn(features), axis=2
        )
        # Concatenate the routing probabilities with their complements.
        decisions = layers.concatenate(
            [decisions, 1 - decisions], axis=2
        )
        # Initiate mu, the probablity of a sample reaching a leaf node
        mu = tf.ones([batch_size, 1, 1])

        begin_idx = 1
        end_idx = 2
        # Traverse the tree in breadth-first order.
        # Update probabilities in each level and node. 
        # Calculate total final output probability
        for level in range(self.depth):
            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]
            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]
            level_decisions = decisions[
                :, begin_idx:end_idx, :
            ]  # [batch_size, 2 ** level, 2]
            mu = mu * level_decisions  # [batch_size, 2**level, 2]
            begin_idx = end_idx
            end_idx = begin_idx + 2 ** (level + 1)
        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]
        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]
        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]
        return outputs

class NeuralDecisionForest(keras.Model):
    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):
        super(NeuralDecisionForest, self).__init__()
        self.ensemble = []
        # Initialize the ensemble by adding NeuralDecisionTree instances.
        # Each tree will have its own randomly selected input features to use.
        for _ in range(num_trees):
            self.ensemble.append(
                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes
                )
            )

    def call(self, inputs):
        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.
        batch_size = tf.shape(inputs)[0]
        outputs = tf.zeros([batch_size, num_classes])

        # Aggregate the outputs of trees in the ensemble.
        for tree in self.ensemble:
            outputs += tree(inputs)
        # Divide the outputs by the ensemble size to get the average.
        outputs /= len(self.ensemble)
        return outputs

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix



path = '/content/drive/MyDrive/MSc/Research/Data/'
train_data_file = "train_data.csv"
test_data_file = "test_data.csv"

# Create the forest model by taking input, output and the model structure
def create_forest_model():
    inputs = create_model_inputs(feature_names)
    features = encode_inputs(inputs)
    features = layers.BatchNormalization()(features)
    num_features = features.shape[1]

    forest_model = NeuralDecisionForest(
        num_trees, depth, num_features, used_features_rate, num_classes
    )

    outputs = forest_model(features)
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model
 
 # Now compile the model, train it on train sample and predict it for test sample.
def run_experiment(model):

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss=keras.losses.SparseCategoricalCrossentropy(),
        metrics=[keras.metrics.SparseCategoricalAccuracy()]
    )

    print("Start training the model...")
    train_dataset = get_dataset_from_csv(
        path+train_data_file, batch_size=batch_size
    )
    print(train_dataset)
    model.fit(train_dataset, epochs=10)
    print("Model training finished")

    print("Evaluating the model on the train data...")
    _, accuracy = model.evaluate(train_dataset)
    print(f"Train accuracy: {round(accuracy * 100, 2)}%")


    print("Evaluating the model on the test data...")
    test_dataset = get_dataset_from_csv(path+test_data_file, batch_size=batch_size)

    _, accuracy = model.evaluate(test_dataset)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")
    evaluation =  model.evaluate(test_dataset)
    print(evaluation)
    return model
forest_model = create_forest_model()
# print(forest_model.summary())
model = run_experiment(forest_model)

test_dataset = get_dataset_from_csv(path+test_data_file, batch_size=batch_size)
y_pred = model.predict(test_dataset)
y_pred = y_pred[:, 0]
y_pred = np.where(y_pred > 0.5, 1,0)
print(y_pred)
print(y_test)

# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(y_test, y_pred)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(y_test, y_pred)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(y_test, y_pred)
print('F1 score: %f' % f1)

# kappa
kappa = cohen_kappa_score(y_test, y_pred)
print('Cohens kappa: %f' % kappa)
# ROC AUC
auc = roc_auc_score(y_test, y_pred)
print('ROC AUC: %f' % auc)
# confusion matrix
matrix = confusion_matrix(y_test, y_pred)
print(matrix)